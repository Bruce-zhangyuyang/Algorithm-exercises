---
title:任务二
---

## 目录
------------------------------------------------
[一、逻辑回归与线性回归的联系与区别](#逻辑回归与线性回归的联系与区别)

[二、逻辑回归原理及损失函数](#逻辑回归原理)

[三、正则化与模型评估指标](#正则化与模型评估指标)

[四、逻辑回归的优缺点](#逻辑回归的优缺点)

[五、样本不均衡问题解决方法](#样本不均衡问题解决方法)

[六、sklearn参数](#sklearn参数)

-------------------------------------------------
# 逻辑回归与线性回归的联系与区别

逻辑回归其本质还是属于线性回归，他仅仅在线性回归的基础上，在特征到结果的映射中加入了一层sigmoid函数（非线性）映射，即先把特征线性求和，然后使用sigmoid函数来预测，使样本能映射到之间的数值，用来做分类问题。




------------------------------------------------
------------------------------------------------
# 逻辑回归原理

如图所示

![逻辑回归](./_img/逻辑回归.jpg)
------------------------------------------------
------------------------------------------------
# 正则化与模型评估指标

模型评估：

训练集误差小（1%），验证集大（15%），为高方差，可能过拟合训练集了；

训练集误差大（15%），验证集大（16%），错误率几乎为0，高偏差，可能欠拟合，识别不准确；

训练集误差小（0.5%），验证集小（1%），低方差，低偏差；

训练集误差小（15%），验证集小（30%），高方差，高偏差；过拟合部分数据；


正则化：

L1范数：向量各个元素绝对值之和。||$x$||$_1=\sum_{i=1}^n$|$x_i$|

L2范数：向量各个元素的平方求和然后求平方根。||$x$||$_2=\sqrt{\sum_{i=1}^n{x_i}^2}$

$L_p$范数：向量各个元素绝对值的p次方求和然后求 1/p 次方。||$x$||$_p=\sqrt[p]{\sum_{i=1}^n{x_i}^2}$

------------------------------------------------
------------------------------------------------
# 逻辑回归的优缺点

优点： 1.速度快，适合二分类问题。2简单易于理解，直接看到各个特征的权重。3.非常容易地更新模型吸收新的数据。

缺点：1.对模型中自变量多重共线性较为敏感，例如两个高度相关自变量同时放入模型，可能导致较弱的一个自变量回归符号不符合预期，符号被扭转。​需要利用因子分析或者变量聚类分析等手段来选择代表性的自变量，以减少候选变量之间的相关性；
2.预测结果呈“S”型，因此从log(odds)向概率转化的过程是非线性的，在两端随着​log(odds)值的变化，概率变化很小，边际值太小，slope太小，而中间概率的变化很大，很敏感。 导致很多区间的变量变化对目标概率的影响没有区分度，无法确定阀值。

------------------------------------------------
------------------------------------------------
# 样本不均衡问题解决方法

所谓的不平衡数据集指的是数据集各个类别的样本量极不均衡。以二分类问题为例，假设正类的样本数量远大于负类的样本数量，通常情况下通常情况下把多数类样本的比例接近100:1这种情况下的数据称为不平衡数据。不平衡数据的学习即需要在分布不均匀的数据集中学习到有用的信息。

不平衡数据集的处理方法主要分为两个方面：

1、从数据的角度出发，主要方法为采样，分为欠采样和过采样以及对应的一些改进方法。

针对不平衡数据, 最简单的一种方法就是生成少数类的样本, 这其中最基本的一种方法就是： 从少数类的样本中进行随机采样来增加新的样本，对应Python库中函数为RandomOverSampler

与过采样相反，欠采样是从多数类样本中随机选择少量样本，再合并原有少数类样本作为新的训练数据集。

随机欠采样有两种类型分别为有放回和无放回两种，无放回欠采样在对多数类某样本被采样后不会再被重复采样，有放回采样则有可能。

解决实际问题的时候，我们用的过采样的情况比较多，所以我一般运用SMOTE。

2、从算法的角度出发，考虑不同误分类情况代价的差异性对算法进行优化，主要是基于代价敏感学习算法(Cost-Sensitive Learning)，代表的算法有adacost；

如果不平衡数据集比例过大，直接可以忽略掉另一个数据的存在，我们便可以把模型用于一分类（One Class Learning）或者异常检测（Novelty Detection）问题，代表的算法有One-class SVM。
------------------------------------------------
------------------------------------------------
# sklearn参数详解


    class sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0,fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None,solver=’liblinear’, max_iter=100, multi_class=’ovr’, verbose=0,warm_start=False, n_jobs=1)
参数详解：

### penalty :默认为l2

**说明**:LogisticRegression默认带了正则化项。penalty参数可选择的值为”l1”和”l2”.分别对应L1的正则化和L2的正则化，默认是L2的正则化。

### dual:默认为false

**说明**:对偶或者原始方法。Dual只适用于正则化相为l2 liblinear的情况，通常样本数大于特征数的情况下，默认为False。

### C:默认为1

**说明**:C为正则化系数λ的倒数，通常默认为1。

### fit_intercept:默认为True

**说明**:是否存在截距，默认存在

### intercept_scaling：default 1

**说明**:仅在正则化项为“liblinear”，且fit_intercept设置为True时有用。


------------------------------------------------
------------------------------------------------

参考：

https://blog.csdn.net/Sakura55/article/details/80776924#2%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E5%92%8C%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E7%9A%84%E5%8C%BA%E5%88%AB

https://blog.csdn.net/zhangxueyang1/article/details/54176359

https://blog.csdn.net/sisteryaya/article/details/81490014 